---
name: ingestor_syslog

packages:
- logstash
- logsearch-config
- java8

templates:
  bin/ingestor_syslog_ctl: bin/ingestor_syslog_ctl
  bin/monit_debugger: bin/monit_debugger
  config/input_and_output.conf.erb: config/input_and_output.conf
  config/filters_pre.conf.erb: config/filters_pre.conf
  config/filters_post.conf.erb: config/filters_post.conf
  config/filters_override.conf.erb: config/filters_override.conf
  config/syslog_tls.crt.erb: config/syslog_tls.crt
  config/syslog_tls.key.erb: config/syslog_tls.key
  config/logstash.yml.erb: config/logstash.yml
  data/properties.sh.erb: data/properties.sh
  helpers/ctl_setup.sh: helpers/ctl_setup.sh
  helpers/ctl_utils.sh: helpers/ctl_utils.sh

provides:
- name: ingestor
  type: ingestor
  properties:
  - logstash_ingestor.syslog.port
  - logstash_ingestor.syslog_tls.port
  - logstash_ingestor.relp.port
- name: syslog_forwarder
  type: syslog_forwarder
  properties:
  - logstash_ingestor.syslog.port

properties:
  logstash.heap_size:
    description: sets jvm heap sized
  logstash.metadata_level:
    description: "Whether to include additional metadata throughout the event lifecycle. NONE = disabled, DEBUG = fully enabled"
    default: "NONE"
  logstash.log_level:
    description: The default logging level (e.g. WARN, DEBUG, INFO)
    default: info
  logstash.plugins:
    description: "Array of hashes describing logstash plugins to install"
    example:
    - {name: logstash-output-cloudwatchlogs, version: 2.0.0}
    default: []

  logstash.queue.type:
    description: Internal queuing model, "memory" for legacy in-memory based queuing and "persisted" for disk-based acked queueing.
    default: persisted
  logstash.queue.page_capacity:
    description: The page data files size. The queue data consists of append-only data files separated into pages.
    default: 250mb
  logstash.queue.max_events:
    description: The maximum number of unread events in the queue.
    default: 0
  logstash.queue.max_bytes:
    description: The total capacity of the queue in number of bytes.
    default: 1024mb
  logstash.queue.checkpoint.acks:
    description: The maximum number of acked events before forcing a checkpoint.
    default: 1024
  logstash.queue.checkpoint.writes:
    description: The maximum number of written events before forcing a checkpoint.
    default: 1024
  logstash.queue.checkpoint.interval:
    description: The interval in milliseconds when a checkpoint is forced on the head page.
    default: 1000

  logstash_ingestor.filters:
    description: Filters to execute on the ingestors
    default: ""

  logstash_ingestor.syslog.port:
    description: Port to listen for syslog messages
    default: 5514

  logstash_ingestor.syslog_tls.port:
    description: Port to listen for syslog-TLS messages (omit to disable)
  logstash_ingestor.syslog_tls.ssl_cert:
    description: Syslog-TLS SSL certificate (file contents, not a path) - required if logstash_ingestor.syslog_tls.port set
  logstash_ingestor.syslog_tls.ssl_key:
    description: Syslog-TLS SSL key (file contents, not a path) - required if logstash_ingestor.syslog_tls.port set
  logstash_ingestor.syslog_tls.skip_ssl_validation:
    description: Verify the identity of the other end of the SSL connection against the CA.
    default: false

  logstash_ingestor.relp.port:
    description: Port to listen for RELP messages
    default: 2514

  logstash_parser.debug:
    description: Debug level logging
    default: false
  logstash_parser.message_max_size:
    description: "Maximum log message length.  Anything larger is truncated (TODO: move this to ingestor?)"
    default: 1048576
  logstash_parser.filters:
    description: "The configuration to embed into the logstash filters section. Can either be a set of parsing rules as a string or a list of hashes in the form of [{name: path_to_parsing_rules.conf}]"
    default: ''
  logstash_parser.deployment_dictionary:
    description: "A list of files concatenated into one deployment dictionary file. Each file contains a hash of job name-deployment name keypairs for @source.deployment lookup."
    default: [ "/var/vcap/packages/logsearch-config/deployment_lookup.yml" ]
  logstash_parser.inputs:
    description: |
      A list of input plugins, with a hash of options for each of them. Please refer to example below.
    example:
      inputs:
        - plugin: rabbitmq
          options:
            host: 192.168.1.1
            user: logsearch
            password: c1oudbunny
  logstash_parser.outputs:
    description: |
      A list of output plugins, with a hash of options for each of them. Please refer to example below.
    example:
      inputs:
        - plugin: mongodb
          options:
            uri: 192.168.1.1
            database: logsearch
            collection: logs
    default: [ { plugin: "elasticsearch", options: {} } ]
  logstash_parser.workers:
    description: "The number of worker threads that logstash should use (default: auto = one per CPU)"
    default: auto
  logstash_parser.elasticsearch.idle_flush_time:
    description: "How frequently to flush events if the output queue is not full."
  logstash_parser.elasticsearch.document_id:
    description: "Use a specific, dynamic ID rather than an auto-generated identifier."
    default: ~
  logstash_parser.elasticsearch.index:
    description: "The specific, dynamic index name to write events to."
    default: "logstash-%{+YYYY.MM.dd}"
  logstash_parser.elasticsearch.index_type:
    description: "The specific, dynamic index type name to write events to."
    default: "%{@type}"
  logstash_parser.elasticsearch.routing:
    description: "The routing to be used when indexing a document."
  logstash_parser.elasticsearch.data_hosts:
    description: The list of elasticsearch data node IPs
    default: [127.0.0.1]
  logstash_parser.elasticsearch.flush_size:
    description: Controls how many logs will be buffered and sent to Elasticsearch for bulk indexing
    default: 500
  logstash_parser.timecop.reject_greater_than_hours:
    description: "Logs with timestamps greater than this many hours in the future won't be parsed and will get tagged with fail/timecop"
    default: 1
  logstash_parser.timecop.reject_less_than_hours:
    description: "Logs with timestamps less than this many hours in the past won't be parsed and will get tagged with fail/timecop"
    default: 24
  logstash_parser.enable_json_filter:
    description: "Toggles the if_it_looks_like_json.conf filter rule"
    default: false
  logstash_parser.wait_for_templates:
    description: "A list of index templates that need to be present in ElasticSearch before the process starts"
    default: ["index_template"]
